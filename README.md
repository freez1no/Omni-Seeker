# Isaac Lab 기반 Jetbot의 YOLO v11 객체 인식을 통한 강화학습 주행

이 프로젝트는 **NVIDIA Isaac Lab** 시뮬레이션 환경에서 **Jetbot** 로봇이 자율적으로 객체를 탐색하고 접근하도록 훈련하는 것을 목표로 합니다.


## 1. 🤖 프로젝트 소개

본 프로젝트는 로봇의 '인식'과 '행동'을 결합하는 강화학습 파이프라인을 구축합니다.

시뮬레이션 환경(Isaac Lab)에서 `Jetbot.usd` 에셋을 불러와 로봇에 탑재된 **카메라 센서**를 통해 실시간으로 주변 환경 데이터를 수집합니다. 이 비전 데이터는 최신 객체 인식 모델인 **YOLO v11**로 전송되어 '사람' 또는 사전에 정의된 '사물'의 위치를 식별합니다.

**강화학습(Reinforcement Learning)** 에이전트는 이 YOLO v11의 탐지 결과를 보상(Reward) 및 상태(State) 정보로 활용하여, 탐지된 객체에게 안전하고 효율적으로 **가까이 다가가는** 최적의 주행 정책(policy)을 학습합니다.

### 🎯 주요 목표

1.  **데이터 수집:** Isaac Lab 내 Jetbot의 카메라 센서로부터 RGB 이미지 데이터를 실시간으로 수집합니다.
2.  **객체 인식:** 수집된 이미지를 YOLO v11 모델로 처리하여 원하는 객체(사람, 사물 등)의 바운딩 박스(bounding box) 정보를 획득합니다.
3.  **강화학습 적용:** 객체 탐지 정보를 기반으로, 로봇이 목표물에 성공적으로 접근했을 때 보상을 제공하는 강화학습 환경을 설계하고 에이전트를 훈련시킵니다.

### 🛠️ 주요 기술 스택

* **시뮬레이터:** NVIDIA Isaac Lab
* **로봇 에셋:** Jetbot.usd
* **객체 인식:** YOLO v11
* **학습 알고리즘:** 강화학습 (PPO)
* **데이터 파이프라인:** Isaac Sim Camera Sensor
